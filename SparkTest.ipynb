{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Demonstration\n",
    "\n",
    "This is a demo of connecting Jupyter to Spark 2.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkHome\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/zack.angelo/Downloads/spark-2.0.0-preview\"\u001b[0m\n",
       "\u001b[36msparkAssembly\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/zack.angelo/Downloads/spark-2.0.0-preview/lib/spark-assembly-2.0.0-preview.jar\"\u001b[0m\n",
       "\u001b[36msparkMaster\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"spark://AUS-MBP-ANGELO:7077\"\u001b[0m\n",
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.0-preview\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkHome = \"/root/spark-2.0.0-preview-bin-hadoop2.7\"\n",
    "val sparkAssembly = s\"$sparkHome/lib/spark-assembly-2.0.0-preview.jar\"\n",
    "val sparkMaster = \"spark://localhost:7077\"\n",
    "val sparkVersion = \"2.0.0-preview\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16 new artifacts in macro\n",
      "16 new artifacts in runtime\n",
      "16 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"com.github.alexarchambault.ammonium\" % s\"spark_2.0.0-preview_2.11.8\" % \"0.4.0-SNAPSHOT\",\n",
    "    \"com.databricks\" % \"spark-csv_2.11\" % \"1.4.0\",\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % \"2.0.0-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Running Spark version 2.0.0-preview\n",
      "16/06/09 08:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/09 08:22:12 INFO SecurityManager: Changing view acls to: zack.angelo\n",
      "16/06/09 08:22:12 INFO SecurityManager: Changing modify acls to: zack.angelo\n",
      "16/06/09 08:22:12 INFO SecurityManager: Changing view acls groups to: \n",
      "16/06/09 08:22:12 INFO SecurityManager: Changing modify acls groups to: \n",
      "16/06/09 08:22:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(zack.angelo); groups with view permissions: Set(); users  with modify permissions: Set(zack.angelo); groups with modify permissions: Set()\n",
      "16/06/09 08:22:12 INFO Utils: Successfully started service 'sparkDriver' on port 54460.\n",
      "16/06/09 08:22:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/06/09 08:22:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/06/09 08:22:12 INFO DiskBlockManager: Created local directory at /private/var/folders/65/tld0p45d1qlg257nq4drzw24rq_dm2/T/blockmgr-f155af63-026c-47fe-bd6a-e1a4665ec13f\n",
      "16/06/09 08:22:12 INFO MemoryStore: MemoryStore started with capacity 2.4 GB\n",
      "16/06/09 08:22:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/06/09 08:22:12 WARN AbstractHandler: No Server set for org.spark_project.jetty.server.handler.ErrorHandler@44ced78b\n",
      "16/06/09 08:22:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/06/09 08:22:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.1.2:4040\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.1/sourcecode_2.11-0.1.1.jar at spark://10.0.1.2:54460/jars/sourcecode_2.11-0.1.1.jar with timestamp 1465478532692\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/jupyter/kernel-api_2.11/0.3.0-M5/kernel-api_2.11-0.3.0-M5.jar at spark://10.0.1.2:54460/jars/kernel-api_2.11-0.3.0-M5.jar with timestamp 1465478532692\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/ammonium/interpreter-api_2.11.8/0.4.0-M6-1/interpreter-api_2.11.8-0.4.0-M6-1.jar at spark://10.0.1.2:54460/jars/interpreter-api_2.11.8-0.4.0-M6-1.jar with timestamp 1465478532692\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/jupyter/scala-api_2.11.8/0.3.0-M3/scala-api_2.11.8-0.3.0-M3.jar at spark://10.0.1.2:54460/jars/scala-api_2.11.8-0.3.0-M3.jar with timestamp 1465478532769\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/ammonium/tprint_2.11.8/0.4.0-M6-1/tprint_2.11.8-0.4.0-M6-1.jar at spark://10.0.1.2:54460/jars/tprint_2.11.8-0.4.0-M6-1.jar with timestamp 1465478532769\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.11/0.3.9/pprint_2.11-0.3.9.jar at spark://10.0.1.2:54460/jars/pprint_2.11-0.3.9.jar with timestamp 1465478532770\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.4/scala-xml_2.11-1.0.4.jar at spark://10.0.1.2:54460/jars/scala-xml_2.11-1.0.4.jar with timestamp 1465478532770\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/lihaoyi/derive_2.11/0.3.9/derive_2.11-0.3.9.jar at spark://10.0.1.2:54460/jars/derive_2.11-0.3.9.jar with timestamp 1465478532877\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar at spark://10.0.1.2:54460/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1465478532878\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.8/scala-compiler-2.11.8.jar at spark://10.0.1.2:54460/jars/scala-compiler-2.11.8.jar with timestamp 1465478532878\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/Library/Jupyter/kernels/scala211/launcher.jar at spark://10.0.1.2:54460/jars/launcher.jar with timestamp 1465478532878\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/cldrdata.jar at spark://10.0.1.2:54460/jars/cldrdata.jar with timestamp 1465478532982\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/dnsns.jar at spark://10.0.1.2:54460/jars/dnsns.jar with timestamp 1465478532982\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/jaccess.jar at spark://10.0.1.2:54460/jars/jaccess.jar with timestamp 1465478532982\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/jfxrt.jar at spark://10.0.1.2:54460/jars/jfxrt.jar with timestamp 1465478532982\n",
      "16/06/09 08:22:12 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/localedata.jar at spark://10.0.1.2:54460/jars/localedata.jar with timestamp 1465478532982\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/nashorn.jar at spark://10.0.1.2:54460/jars/nashorn.jar with timestamp 1465478533088\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/sunec.jar at spark://10.0.1.2:54460/jars/sunec.jar with timestamp 1465478533088\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar at spark://10.0.1.2:54460/jars/sunjce_provider.jar with timestamp 1465478533088\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar at spark://10.0.1.2:54460/jars/sunpkcs11.jar with timestamp 1465478533088\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre/lib/ext/zipfs.jar at spark://10.0.1.2:54460/jars/zipfs.jar with timestamp 1465478533088\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/System/Library/Java/Extensions/MRJToolkit.jar at spark://10.0.1.2:54460/jars/MRJToolkit.jar with timestamp 1465478533194\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.4.0/spark-csv_2.11-1.4.0.jar at spark://10.0.1.2:54460/jars/spark-csv_2.11-1.4.0.jar with timestamp 1465478533194\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar at spark://10.0.1.2:54460/jars/javax.servlet-3.0.0.v201112011016.jar with timestamp 1465478533194\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar at spark://10.0.1.2:54460/jars/jetty-server-8.1.14.v20131031.jar with timestamp 1465478533194\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-continuation/8.1.14.v20131031/jetty-continuation-8.1.14.v20131031.jar at spark://10.0.1.2:54460/jars/jetty-continuation-8.1.14.v20131031.jar with timestamp 1465478533304\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.ivy2/local/com.github.alexarchambault.ammonium/shell-api_2.11.8/0.4.0-SNAPSHOT/jars/shell-api_2.11.8.jar at spark://10.0.1.2:54460/jars/shell-api_2.11.8.jar with timestamp 1465478533305\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar at spark://10.0.1.2:54460/jars/commons-csv-1.1.jar with timestamp 1465478533305\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar at spark://10.0.1.2:54460/jars/jetty-io-8.1.14.v20131031.jar with timestamp 1465478533305\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar at spark://10.0.1.2:54460/jars/jetty-util-8.1.14.v20131031.jar with timestamp 1465478533415\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/8.1.14.v20131031/jetty-http-8.1.14.v20131031.jar at spark://10.0.1.2:54460/jars/jetty-http-8.1.14.v20131031.jar with timestamp 1465478533415\n",
      "16/06/09 08:22:13 INFO Spark$SparkContext: Added JAR file:/Users/zack.angelo/.ivy2/local/com.github.alexarchambault.ammonium/spark_2.0.0-preview_2.11.8/0.4.0-SNAPSHOT/jars/spark_2.0.0-preview_2.11.8.jar at spark://10.0.1.2:54460/jars/spark_2.0.0-preview_2.11.8.jar with timestamp 1465478533415\n",
      "16/06/09 08:22:13 INFO AppClient$ClientEndpoint: Connecting to master spark://AUS-MBP-ANGELO:7077...\n",
      "16/06/09 08:22:13 INFO TransportClientFactory: Successfully created connection to AUS-MBP-ANGELO/10.0.1.2:7077 after 28 ms (0 ms spent in bootstraps)\n",
      "16/06/09 08:22:13 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160609082213-0000\n",
      "16/06/09 08:22:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54462.\n",
      "16/06/09 08:22:13 INFO NettyBlockTransferService: Server created on 10.0.1.2:54462\n",
      "16/06/09 08:22:13 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/06/09 08:22:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:54462 with 2.4 GB RAM, BlockManagerId(driver, 10.0.1.2, 54462)\n",
      "16/06/09 08:22:13 INFO BlockManagerMaster: Registered BlockManager\n",
      "16/06/09 08:22:13 INFO AppClient$ClientEndpoint: Executor added: app-20160609082213-0000/0 on worker-20160609082053-10.0.1.2-54210 (10.0.1.2:54210) with 8 cores\n",
      "16/06/09 08:22:13 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160609082213-0000/0 on hostPort 10.0.1.2:54210 with 8 cores, 1024.0 MB RAM\n",
      "16/06/09 08:22:13 INFO AppClient$ClientEndpoint: Executor updated: app-20160609082213-0000/0 is now RUNNING\n",
      "16/06/09 08:22:13 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mSpark\u001b[0m: \u001b[32mammonite\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSpark\u001b[0m = Spark\n",
       "\u001b[36mres2_1\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@5ad08203\n",
       "\u001b[36mres2_2\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = SparkContext"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val Spark = new ammonite.spark.Spark\n",
    "\n",
    "Spark.sparkConf\n",
    "    .setMaster(\"spark://localhost:7077\")\n",
    "    .setAppName(\"ammonium-spark\")\n",
    "    .set(\"spark.home\", sparkHome)\n",
    "\n",
    "Spark.sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mSpark.{ sparkConf, sc, sqlContext }\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.RandomForestClassifier\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, VectorAssembler, StringIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.{DataFrame, Row, SQLContext}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Spark.{ sparkConf, sc, sqlContext } //Imports must be in separate cell, at the top\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainFile\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = titanic/train.csv MapPartitionsRDD[1] at textFile at Main.scala:25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val trainFile = sc.textFile(\"titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m]] = MapPartitionsRDD[2] at map at Main.scala:25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val trainData = trainFile.map(_.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mloadData\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadData(\n",
    "  trainFile: String,\n",
    "  testFile: String,\n",
    "  sqlContext: org.apache.spark.sql.SQLContext\n",
    "): (org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame) = {\n",
    "  val nullable = true\n",
    "  val schemaArray = Array(\n",
    "    StructField(\"PassengerId\", IntegerType, nullable),\n",
    "    StructField(\"Survived\", IntegerType, nullable),\n",
    "    StructField(\"Pclass\", IntegerType, nullable),\n",
    "    StructField(\"Name\", StringType, nullable),\n",
    "    StructField(\"Sex\", StringType, nullable),\n",
    "    StructField(\"Age\", FloatType, nullable),\n",
    "    StructField(\"SibSp\", IntegerType, nullable),\n",
    "    StructField(\"Parch\", IntegerType, nullable),\n",
    "    StructField(\"Ticket\", StringType, nullable),\n",
    "    StructField(\"Fare\", FloatType, nullable),\n",
    "    StructField(\"Cabin\", StringType, nullable),\n",
    "    StructField(\"Embarked\", StringType, nullable)\n",
    "  )\n",
    "\n",
    "  val trainSchema = StructType(schemaArray)\n",
    "  val testSchema = StructType(schemaArray.filter(p => p.name != \"Survived\"))\n",
    "\n",
    "  val csvFormat = \"com.databricks.spark.csv\"\n",
    "\n",
    "  val trainDF = sqlContext.read\n",
    "  .format(csvFormat)\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(trainSchema)\n",
    "  .load(trainFile)\n",
    "\n",
    "  val testDF = sqlContext.read\n",
    "  .format(csvFormat)\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(testSchema)\n",
    "  .load(testFile)\n",
    "\n",
    "  (trainDF, testDF)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainDf\u001b[0m: \u001b[32mDataFrame\u001b[0m = [PassengerId: int, Survived: int ... 10 more fields]\n",
       "\u001b[36mtestDf\u001b[0m: \u001b[32mDataFrame\u001b[0m = [PassengerId: int, Pclass: int ... 9 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val (trainDf,testDf) = loadData(\"titanic/train.csv\", \"titanic/test.csv\", sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|     |       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583|     |       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|     |       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|     |       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|     |       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|     |       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|     |       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|     |       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|     |       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|     |       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0|     |       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|     |       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225|     |       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:26: type mismatch;",
      " found   : org.apache.spark.sql.Column",
      " required: org.apache.spark.sql.Row => ?",
      "trainDf.groupByKey(col(\"Sex\"))",
      "                      ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "trainDf.groupByKey(col(\"Sex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down (Job 1 cancelled because SparkContext was shut down)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:816)",
      "  scala.collection.mutable.HashSet.foreach(HashSet.scala:78)",
      "  org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:816)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1677)",
      "  org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)",
      "  org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1600)",
      "  org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1771)",
      "  org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)",
      "  org.apache.spark.SparkContext.stop(SparkContext.scala:1770)",
      "  org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:564)",
      "  org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:215)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1793)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)",
      "  scala.util.Try$.apply(Try.scala:192)",
      "  org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)",
      "  org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)",
      "  org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1863)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1876)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1889)",
      "  org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)",
      "  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2122)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)",
      "  org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2436)",
      "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2121)",
      "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2128)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1862)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1861)",
      "  org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2466)",
      "  org.apache.spark.sql.Dataset.head(Dataset.scala:1861)",
      "  org.apache.spark.sql.Dataset.take(Dataset.scala:2078)",
      "  org.apache.spark.sql.Dataset.showString(Dataset.scala:240)",
      "  org.apache.spark.sql.Dataset.show(Dataset.scala:533)",
      "  org.apache.spark.sql.Dataset.show(Dataset.scala:493)",
      "  cmd11$$user$$anonfun$1.apply$mcV$sp(Main.scala:25)"
     ]
    }
   ],
   "source": [
    "trainDf.select(\"PassengerId\", \"Survived\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
